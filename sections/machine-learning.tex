\section{Machine Learning}\label{sec:ai}
Content can be suggested in different ways.
Instead of only displaying content based on its publication date, it makes more sense to only suggest content that is relevant to the user.
But it is difficult to say what is relevant for a user and what is not, because in the end this should be decided by the user.
Because this is difficult to do by hand, this process should be supported by a machine learning algorithm.
A machine learning algorithm is basically an algorithm that can adapt results based on data inputs and thus learn more about results step by step.\\

Such an algorithm is also used by Twitter, for example\cite{twitter-deep-learning}.
Twitter itself even uses a subset of machine learning, namely deep learning.
Roughly speaking, even more information is processed here in different layers in the neural network.
In order to suggest posts, a lot of different data is collected in order to determine the relevance for the respective users.
There all interactions, previous interactions between user and post author or previous preferences are summarized and evaluated.\\

The difficult thing is that problems can arise in many places with such an approach.
Particular problem areas are the different machine learning steps: input, processing and output.
In order to keep these problems to a minimum, certain values are tracked, similar to value sensitive design.
These are fairness, accountability, transparency and ethics.
First, it is difficult to ensure all these values perfectly.
Similar to value sensitive design, however, it is better to think about such values than to ignore them completely.\\

Fairness can be ensured if students are interviewed from as many categories as possible.
In doing so, prejudices should be eliminated and all categories of students should be treated equally\cite{fairness-framework}.
This could be characterized by the fact that people who insult on the platform UK4You both are punished equally for the same insult.
On the other hand, it should also be noted that people with disadvantages are also taken into account.
Insults that occur as a tic due to Tourette's neuropsychiatric illness cannot be punished in the same way as deliberate active insults.\\

Basically, making machine learning accountable means that you have to consider what effects can arise from the use of machine learning and who is accountable for it.
It should be noted that machine learning always takes place within the legal framework and does not violate any legal guidelines.
The important thing about accountability is that a team of experts can initiate measures in the event of problems.
For example, let's assume that an algorithm ensures that a person is constantly shown objectionable content on the UK4You platform.
Despite setting different filters, it is not possible to get rid of them.
However, the person could not continue to use the platform because this content triggers the user in some way.
The logical consequence of this would be that a team of experts can intervene in this algorithm in order to filter the content.
Thus, the user would no longer be excluded from the platform by the triggering content.\\

Another point is to make processes transparent. 
This applies not only to users, but also to designers or other stakeholders.
Let's take the example from earlier where the expert team wants to filter content.
When the algorithm works as a black box where only input and output are known, no easy adjustments can be made.
However, if the algorithm were transparent, developers could easily locate the problem area.
But transparency is also important for users. 
Suggesting content to users without conveying what it is based on means that users are less able to determine what they consume.
If every interaction can trigger certain processes inside a black box, this ensures that users unconsciously determine their own content without possibly wanting to.
It is also unclear which interaction is responsible for a change at all.
For this reason, this should be communicated clearly. 
The algorithm should be made explainable.\\

UK4You should be a platform that meets current ethical standards.
Therefore, before using the UK4You platform, the ethical committee of the University of Kassel should be considered.
It is also necessary to consider which data is used for the learning process.
As already mentioned, there are three central points in the learning process.
And it is precisely in these three problem areas that different forms of bias can occur\cite{framework-understanding-ml}.\\

Depending on the data collection, it can already happen that smaller groups of people are excluded or stereotypes are reproduced.
For example, using only the data from the five people interviewed could cause one or both of the problems just mentioned to occur.
In this small sample size, only people in a certain age range or people who identify as male were interviewed.
This can also be referred to as representational bias.
Representational bias states that the selected sample size underrepresents the entire group.
Preferences of a younger generation or a different gender are not always easily transferrable to others.
It would therefore be important to collect as many data sets as possible on the students at the University of Kassel.
However, these would have to be re-evaluated by a team of experts and the ethics committee to ensure that no stereotypes are encouraged or other problem cases arise.\\

Another form of bias is the measurement bias.
This states that it can happen that complex constructs are abstracted too much and important correlations can be lost.
It can also happen that one group is observed more closely than another because of its error rate.
Due to the additional measurements, this group then goes through more feedback loops compared to the others.
It is important to define in advance which goals machine learning should have and which values make sense for achieving this goal.\\

Similar to the measurement bias, an aggregation bias can also lead to misunderstandings.
Information can have different meanings depending on the context.
So it may be that for two students, not using emojis has a completely different meaning.
For example, a student might use emojis regularly to symbolize casual interactions.
However, if he avoids the emojis, this should convey a serious approach.
The other student, on the other hand, just doesn't like emojis and doesn't use them regardless of mood or seriousness.
Regular surveys via the UK4You platform could help to identify such problems more easily.\\

Deployment bias describes the incorrect interpretation of the result and the resulting consequences.
For example, iterating actions on the platform should in no way reflect the evaluation of students.
Likewise, no other evaluations of machine learning should have an impact on decisions.
To prevent this, teachers should attend a seminar on how to deal with unconscious grade decisions.\\

There are many other forms of bias that are not explained in detail here.
In any case, it is also important to be aware of these during the design of UK4You.
Countermeasures and various tests should also be taken in advance in order to design an optimal platform for students.
